{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This lesson is a brief introduction to TOAST and its data representations.  This next cell is just initializing some things for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are you using a special reservation for a workshop?\n",
    "# If so, set it here:\n",
    "nersc_reservation = None\n",
    "\n",
    "# Load common tools for all lessons\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from lesson_tools import (\n",
    "    check_nersc,\n",
    "    fake_focalplane\n",
    ")\n",
    "nersc_host, nersc_repo = check_nersc(reservation=nersc_reservation)\n",
    "if nersc_host is not None:\n",
    "    %reload_ext slurm_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Environment\n",
    "\n",
    "You can get the current TOAST runtime configuration from the \"Environment\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast\n",
    "\n",
    "env = toast.Environment.get()\n",
    "# FIXME:  update __repr__ of Environment class\n",
    "print(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model\n",
    "\n",
    "Before using TOAST for simulation or analysis, it is important to discuss how data is stored in memory and how that data can be distributed among many processes to parallelize large workflows.\n",
    "\n",
    "First, let's create a fake focalplane of detectors to use throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate a fake focalplane with 7 pixels, each with 2 detectors.\n",
    "\n",
    "fp = fake_focalplane()\n",
    "\n",
    "# Make a plot of this focalplane layout.\n",
    "\n",
    "detnames = list(sorted(fp.keys()))\n",
    "detquat = {x: fp[x][\"quat\"] for x in detnames}\n",
    "detfwhm = {x: fp[x][\"fwhm_arcmin\"] for x in detnames}\n",
    "detlabels = {x: x for x in detnames}\n",
    "detpolcol = {x: \"red\" if i % 2 == 0 else \"blue\" for i, x in enumerate(detnames)}\n",
    "\n",
    "toast.tod.plot_focalplane(\n",
    "    detquat, 4.0, 4.0, None, fwhm=detfwhm, polcolor=detpolcol, labels=detlabels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations with Time Ordered Data\n",
    "\n",
    "TOAST works with data organized into *observations*.  Each observation is independent of any other observation.  An observation consists of co-sampled detectors for some span of time.  The intrinsic detector noise is assumed to be stationary within an observation.  Typically there are other quantities which are constant for an observation (e.g. elevation, weather conditions, satellite spin axis, etc).\n",
    "\n",
    "An observation is just a dictionary with at least one member (\"tod\") which is an instance of a class that derives from the `toast.TOD` base class.\n",
    "\n",
    "The inputs to a TOD class constructor are at least:\n",
    "\n",
    "1. The detector names for the observation.\n",
    "2. The number of samples in the observation.\n",
    "3. The geometric offset of the detectors from the boresight.\n",
    "4. Information about how detectors and samples are distributed among processes.  More on this below.\n",
    "\n",
    "The TOD class can act as a storage container for different \"flavors\" of timestreams as well as a source and sink for the observation data (with the read_\\*() and write_\\*() methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.qarray as qa\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "obs = dict()\n",
    "obs[\"name\"] = \"20191014_000\"\n",
    "\n",
    "# The type of TOD class is usually specific to the data processing job.\n",
    "# For example it might be one of the simulation classes or it might be\n",
    "# a class that loads experiment data.  Here we just use a simple class\n",
    "# that is only used for testing and which reads / writes data to internal memory\n",
    "# buffers.\n",
    "\n",
    "tod = toast.tod.TODCache(None, detnames, nsamples, detquats=detquat)\n",
    "obs[\"tod\"] = tod\n",
    "\n",
    "# The TOD class has methods to get information about the data:\n",
    "\n",
    "print(\"TOD has detectors {}\".format(\", \".join(tod.detectors)))\n",
    "print(\"TOD has {} total samples for each detector\".format(tod.total_samples))\n",
    "\n",
    "# Write some data.  Not every TOD derived class supports writing (for example,\n",
    "# TOD classes that represent simulations).\n",
    "\n",
    "t_delta = 1.0 / fp[detnames[0]][\"rate\"]\n",
    "tod.write_times(stamps=np.arange(0.0, nsamples * t_delta, t_delta))\n",
    "tod.write_boresight(\n",
    "    data=qa.from_angles(\n",
    "        (np.pi / 2) * np.ones(nsamples),\n",
    "        (2 * np.pi / nsamples) * np.arange(nsamples),\n",
    "        np.zeros(nsamples)\n",
    "    )\n",
    ")\n",
    "for d in detnames:\n",
    "    tod.write(detector=d, data=np.random.normal(scale=fp[d][\"NET\"], size=nsamples))\n",
    "    tod.write_flags(detector=d, flags=np.zeros(nsamples, dtype=np.uint8))\n",
    "\n",
    "# Read it back\n",
    "\n",
    "print(\"TOD timestampes = {} ...\".format(tod.read_times()[:5]))\n",
    "print(\"TOD boresight = \\n{} ...\".format(tod.read_boresight()[:5,:]))\n",
    "for d in detnames:\n",
    "    print(\"TOD detector {} = {} ...\".format(d, tod.read(detector=d, n=5)))\n",
    "    print(\"TOD detector {} flags = {} ...\".format(d, tod.read_flags(detector=d, n=5)))\n",
    "\n",
    "# Store some data in the cache.  The \"cache\" member variable looks like a dictionary of\n",
    "# numpy arrays, but the memory used is allocated in C, so that we can actually clear\n",
    "# these buffers when needed.\n",
    "\n",
    "for d in detnames:\n",
    "    processed = tod.read(detector=d)\n",
    "    processed /= 2.0\n",
    "    # By convention, we usually name buffers in the cache by <prefix>_<detector>\n",
    "    tod.cache.put(\"processed_{}\".format(d), processed)\n",
    "print(\"TOD cache now contains {} bytes\".format(tod.cache.report(silent=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comm : Groups of Processes\n",
    "\n",
    "A toast.Comm instance takes the global number of processes available (MPI.COMM_WORLD) and divides them into groups.  Each process group is assigned one or more observations.  Since observations are independent, this means that different groups can be independently working on separate observations in parallel.  It also means that inter-process communication needed when working on a single observation can occur with a smaller set of processes.\n",
    "\n",
    "At NERSC, this notebook is running on a login node, so we cannot use MPI.  Constructing a default `toast.Comm` whenever MPI use is disabled will just produce a single group of one process.  See the parallel example at the end of this notebook for a case with multiple groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = toast.Comm()\n",
    "# FIXME:  update __repr__ of Comm class\n",
    "print(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : a Collection of Observations\n",
    "\n",
    "A toast.Data instance is mainly just a list of observations.  However remember that each process group will have a different list.  Since we have only one group of one process, this example is not so interesting.  See the parallel case at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data(comm)\n",
    "data.obs.append(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Recapping previous sections, we have some groups of processes, each of which has a set of observations.  Within a single process group, the detector data is distributed across the processes within the group.  That distribution is controlled by the size of the communicator passed to the TOD class, and also by the `detranks` parameter of the constructor.  This detranks number sets the dimension of the process grid in the detector direction.  For example, a value of \"1\" means that every process has all detectors for some span of time.  A value equal to the size of the communicator results in every process having some number of detectors for the entire observation.  The detranks parameter must divide evenly into the number of processes in the communicator and determines how the processes are arranged in a grid.\n",
    "\n",
    "As a concrete example, imagine that MPI.COMM_WORLD has 24 processes.  We split this into 4 groups of 6 procesess.  There are 6 observations of varying lengths and every group has one or 2 processes.  Here is a picture of what data each process would have.  The global process number is shown as well as the rank within the group:\n",
    "<img src=\"toast_data_dist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in Parallel\n",
    "\n",
    "The NERSC login nodes do not support MPI, so all of the previous examples are running serially on a login node.  To run in parallel, we can submit a batch job or get an interactive session on one or more compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile intro.py\n",
    "\n",
    "import toast\n",
    "from toast.mpi import MPI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import toast.qarray as qa\n",
    "\n",
    "\n",
    "# Runtime Environment\n",
    "\n",
    "env = toast.Environment.get()\n",
    "if MPI.COMM_WORLD.rank == 0:\n",
    "    env.print()\n",
    "\n",
    "# Create a fake focalplane\n",
    "\n",
    "fp = fake_focalplane()\n",
    "\n",
    "detnames = list(sorted(fp.keys()))\n",
    "detquat = {x: fp[x][\"quat\"] for x in detnames}\n",
    "detfwhm = {x: fp[x][\"fwhm_arcmin\"] for x in detnames}\n",
    "detlabels = {x: x for x in detnames}\n",
    "detpolcol = {x: \"red\" if i % 2 == 0 else \"blue\" for i, x in enumerate(detnames)}\n",
    "\n",
    "if MPI.COMM_WORLD.rank == 0:\n",
    "    outfile = \"intro_focalplane.pdf\"\n",
    "    toast.tod.plot_focalplane(\n",
    "        detquat, 4.0, 4.0, outfile, fwhm=detfwhm, polcolor=detpolcol, labels=detlabels\n",
    "    )\n",
    "\n",
    "# The TOD base and derived classes\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "obs = dict()\n",
    "\n",
    "# The type of TOD class is usually specific to the data processing job.\n",
    "# For example it might be one of the simulation classes or it might be\n",
    "# a class that loads experiment data.  Here we just use a simple class\n",
    "# that is only used for testing and which reads / writes data to internal memory\n",
    "# buffers.\n",
    "\n",
    "tod = toast.tod.TODCache(MPI.COMM_WORLD, detnames, nsamples, detquats=detquat)\n",
    "obs[\"tod\"] = tod\n",
    "\n",
    "# The TOD class has methods to get information about the data:\n",
    "\n",
    "print(\"TOD has detectors {}\".format(\", \".join(tod.detectors)))\n",
    "print(\"TOD has {} total samples for each detector\".format(tod.total_samples))\n",
    "\n",
    "# Write some data.  Not every TOD derived class supports writing (for example,\n",
    "# TOD classes that represent simulations).\n",
    "\n",
    "t_delta = 1.0 / fp[detnames[0]][\"rate\"]\n",
    "tod.write_times(stamps=np.arange(0.0, nsamples * t_delta, t_delta))\n",
    "tod.write_boresight(\n",
    "    data=qa.from_angles(\n",
    "        (np.pi / 2) * np.ones(nsamples),\n",
    "        (2 * np.pi / nsamples) * np.arange(nsamples),\n",
    "        np.zeros(nsamples)\n",
    "    )\n",
    ")\n",
    "for d in detnames:\n",
    "    tod.write(detector=d, data=np.random.normal(scale=fp[d][\"NET\"], size=nsamples))\n",
    "    tod.write_flags(detector=d, flags=np.zeros(nsamples, dtype=np.uint8))\n",
    "\n",
    "# Read it back\n",
    "\n",
    "print(\"TOD timestampes = {} ...\".format(tod.read_times()[:5]))\n",
    "print(\"TOD boresight = \\n{} ...\".format(tod.read_boresight()[:5,:]))\n",
    "for d in detnames:\n",
    "    print(\"TOD detector {} = {} ...\".format(d, tod.read(detector=d, n=5)))\n",
    "    print(\"TOD detector {} flags = {} ...\".format(d, tod.read_flags(detector=d, n=5)))\n",
    "\n",
    "# Store some data in the cache.  The \"cache\" member variable looks like a dictionary of\n",
    "# numpy arrays, but the memory used is allocated in C, so that we can actually clear\n",
    "# these buffers when needed.\n",
    "\n",
    "for d in detnames:\n",
    "    processed = tod.read(detector=d)\n",
    "    processed /= 2.0\n",
    "    # By convention, we usually name buffers in the cache by <prefix>_<detector>\n",
    "    tod.cache.put(\"processed_{}\".format(d), processed)\n",
    "print(\"TOD cache now contains {} bytes\".format(tod.cache.report(silent=True)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nersc_host is not None:\n",
    "    %srun -N 1 -C knl -n 32 -c 2 --cpu_bind=cores -t 00:03:00 python intro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
